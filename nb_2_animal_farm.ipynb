{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Progetto di Linguistica Computazionale di Giacomo Fidone (531668) –  Estrazione di informazioni da animal_farm.txt"
      ],
      "metadata": {
        "id": "Y7MnArkbjJ9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "_CUl_SYni1Ma"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eEmFlbuQil6Z"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import ngrams\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvH4m9zSi4Jv",
        "outputId": "08eac4d1-6784-42c6-d256-f2721f771e39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estrazione di Informazioni da un testo"
      ],
      "metadata": {
        "id": "9EjyA0o_jmGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Analyze:\n",
        "\n",
        "    def __init__(self, path):\n",
        "        '''Definisce una classe per l'estrazione di informazioni da un testo.'''\n",
        "\n",
        "        self.text = self.read_file(path)\n",
        "        self.sents = nltk.tokenize.sent_tokenize(self.text)\n",
        "        self.tokenized_sents = [nltk.word_tokenize(sent) for sent in self.sents]\n",
        "        self.tokens = [token for sent in self.tokenized_sents for token in sent]\n",
        "        self.freq_dist = nltk.FreqDist(self.tokens)\n",
        "        self.pos = nltk.tag.pos_tag(self.tokens, tagset='universal') # uso il tagset di UD\n",
        "        self.only_pos = [pos for token, pos in self.pos]\n",
        "\n",
        "    def top_k_words(self, k, pos_tags=None):\n",
        "        '''Mostra i k token più frequenti (e relativa frequenza) in ordine decrescente. E' possibile indicare i pos-tags da\n",
        "        considerare assegnando a \"pos_tags\" la lista di UD pos tags. '''\n",
        "\n",
        "        if pos_tags==None:\n",
        "            freqs = self.freq_dist\n",
        "        else:\n",
        "            tokens = [token for token, pos in self.pos if pos in pos_tags]\n",
        "            freqs = nltk.FreqDist(tokens)\n",
        "\n",
        "        top_k = {token : freqs[token] for token in list(freqs)[:k]}\n",
        "        for i, token in enumerate(top_k):\n",
        "            print(f'{i+1}. {token} : {top_k[token]}')\n",
        "\n",
        "    def top_k_ngrams(self, k, n, pos=False):\n",
        "        '''Mostra i primi k n-grammi di parole più frequenti (e relative frequenze) in ordine decrescente. Se \"pos\"=True,\n",
        "        considera n-grammi di pos.'''\n",
        "\n",
        "        if pos:\n",
        "            n_grams = list(ngrams(self.only_pos, n))\n",
        "        else:\n",
        "            n_grams = list(ngrams(self.tokens, n))\n",
        "        voc = list(set(n_grams))\n",
        "        freqs = nltk.FreqDist(n_grams)\n",
        "\n",
        "        top_k = {n_gram : freqs[n_gram] for n_gram in list(freqs)[:k]}\n",
        "        for i, n_gram in enumerate(top_k):\n",
        "            print(f'{i+1}. {n_gram} : {top_k[n_gram]}')\n",
        "\n",
        "    def top_k_bigrams(self, k, pos_tags, order='frequency', display_output=True):\n",
        "        '''Mostra i primi k bigrammi di parole, le cui pos corrispondono a \"pos_tags\", in base a \"order\". Valori possibili di \"order\" sono: \"frequency\" (ordinati per frequenza – default);\n",
        "        \"cond_prob\" (ordinati per probabilità condizionata), \"joint_prob\" (ordinati per probabilità congiunta), \"mi\" (ordinati per mutual information),\n",
        "        \"lmi\" (ordinati per local mutual information). Il metodo è usato internamente da self.common_mi_lmi(), che disattiva il flag \"display_output\".'''\n",
        "\n",
        "        bigrams = list(ngrams(self.tokens, 2))\n",
        "        pos_bigrams = list(ngrams(self.only_pos, 2))\n",
        "        bigrams = [bigram for bigram, pos_bigram in zip(bigrams, pos_bigrams) if pos_bigram==pos_tags] # seleziono i bigrammi le cui pos corrispodono a \"pos_tags\"\n",
        "        bigrams_freq_dist = nltk.FreqDist(bigrams)\n",
        "\n",
        "        results = dict() # dizionario della forma {bigramma:valore}, dove valore è il valore della misura selezionata per \"order\"\n",
        "        if order == 'frequency':\n",
        "            results = bigrams_freq_dist\n",
        "        else:\n",
        "            voc = list(set(bigrams)) # vocabolario di bigrammi\n",
        "            for bigram in voc:\n",
        "                bigram_freq = bigrams_freq_dist[bigram]\n",
        "                first_token_freq = self.freq_dist[bigram[0]]\n",
        "                corpus_size = len(self.tokens)\n",
        "                if order == 'cond_prob' or order=='joint_prob':\n",
        "                    cond_prob = bigram_freq / first_token_freq # P(B|A) = F(A, B) / F(A)\n",
        "                    if order == 'cond_prob':\n",
        "                        results[bigram] = cond_prob\n",
        "                    else:\n",
        "                        results[bigram] = (first_token_freq/corpus_size) * cond_prob # P(A, B) = P(A)P(B|A)\n",
        "                else:\n",
        "                    first_token_prob = first_token_freq / corpus_size\n",
        "                    second_token_prob = self.freq_dist[bigram[1]] / corpus_size\n",
        "                    bigram_prob = bigram_freq / len(bigrams)\n",
        "                    mi = math.log(bigram_prob / (first_token_prob * second_token_prob), 2) # MI = log_2(P(A, B) / P(A) * P(B))\n",
        "                    if order == 'mi':\n",
        "                        results[bigram] = mi\n",
        "                    elif order == 'lmi':\n",
        "                        results[bigram] = bigram_freq * mi # LMI = F(A, B) * MI\n",
        "                    else:\n",
        "                        raise ValueError(f'Invalid value for \"order\". Expected one of [\"frequency\", \"cond_prob\", \"joint_prob\", \"mi\", \"lmi\"], got \"{order}\".')\n",
        "            results = dict(sorted(results.items(), key=lambda item: item[1], reverse=True)) # ordino i bigrammi in senso decrescente secondo il valore assegnato\n",
        "\n",
        "        top_k = {bigram : results[bigram] for bigram in list(results)[:k]} # considero solo i primi k bigrammi\n",
        "        if display_output:\n",
        "            for i, bigram in enumerate(top_k):\n",
        "                print(f'{i+1}. {bigram} : {top_k[bigram]}')\n",
        "        else:\n",
        "            return top_k.keys()\n",
        "\n",
        "    def common_mi_lmi(self, k, pos_tags, display_output=False):\n",
        "        '''Mostra i bigrammi in comune ai primi k bigrammi (le cui pos corrispondono a 'pos_tags') ordinati per mutual information e local mutual information.'''\n",
        "\n",
        "        mi_bigrams = self.top_k_bigrams(k, pos_tags, order='mi', display_output=False)\n",
        "        lmi_bigrams = self.top_k_bigrams(k, pos_tags, order='lmi', display_output=False)\n",
        "        common = [bigram for bigram in mi_bigrams if bigram in lmi_bigrams]\n",
        "        if common:\n",
        "            print(f'Bigrammi in comune nei primi {k} bigrammi con pos {pos_tags} ordinati per mutual information e local mutual information:')\n",
        "            for bigram in common:\n",
        "                print(bigram)\n",
        "        else:\n",
        "            print(f'Non ci sono bigrammi in comune nei primi {k} bigrammi con pos {pos_tags} ordinati per mutual information e local mutual information.')\n",
        "\n",
        "    def analyze_sentences(self):\n",
        "        '''Mostra la frase con la media della distribuzione di frequenza massima, la frase con la media di distribuzione di frequenza minima\n",
        "        e la frase con la più alta probabilità secondo un modello di markov di ordine 2. Considera solo frasi la cui lunghezza è compresa tra 10 e 20 token\n",
        "        e di cui almeno metà dei token non è un hapax.'''\n",
        "\n",
        "        target_sents = [sent for sent in self.tokenized_sents if len(sent) <=20 and len(sent) >= 10] # considero solo frasi la cui lunghezza è compresa tra 10 e 20\n",
        "        for sent in target_sents:\n",
        "            no_hapax_words = len([token for token in sent if self.freq_dist[token] > 1]) # calcolo il numero di parole della frase con frequenza maggiore di 1\n",
        "            if no_hapax_words < (len(sent) // 2): # rimuovo la frase se il numero di parole che non sono hapax è inferiore alla metà del numero di parole totale\n",
        "                target_sents.remove(sent)\n",
        "\n",
        "        max_avg_freq = 0\n",
        "        max_avg_freq_sent = str()\n",
        "        min_avg_freq = float('inf')\n",
        "        min_avg_freq_sent = str()\n",
        "        max_markov_prob = 0\n",
        "        max_markov_prob_sent = str()\n",
        "\n",
        "        for sent in target_sents:\n",
        "            token_freqs = [self.freq_dist[token] for token in sent]\n",
        "            avg_freq = sum(token_freqs) / len(sent) # media della distribuzione di frequenza\n",
        "            markov_prob = self.markov2(sent) # probabilità secondo modello di markov di ordine 2\n",
        "            if avg_freq > max_avg_freq:\n",
        "                max_avg_freq = avg_freq\n",
        "                max_avg_freq_sent = sent\n",
        "            if avg_freq < min_avg_freq:\n",
        "                min_avg_freq = avg_freq\n",
        "                min_avg_freq_sent = sent\n",
        "            if markov_prob > max_markov_prob:\n",
        "                max_markov_prob = markov_prob\n",
        "                max_markov_prob_sent = sent\n",
        "\n",
        "        max_avg_freq_sent = self.sents[self.tokenized_sents.index(max_avg_freq_sent)] # recupero le frasi non tokenizzate per la stampa a video\n",
        "        min_avg_freq_sent = self.sents[self.tokenized_sents.index(min_avg_freq_sent)]\n",
        "        max_markov_prob_sent = self.sents[self.tokenized_sents.index(max_markov_prob_sent)]\n",
        "\n",
        "        print(f'Frase con la media della distribuzione di frequenza dei token più alta ({round(max_avg_freq, 2)}): \"{max_avg_freq_sent}\".', )\n",
        "        print(f'Frase con la media della distribuzione di frequenza dei token più bassa ({round(min_avg_freq, 2)}): \"{min_avg_freq_sent}\".', )\n",
        "        print(f'Frase con la probabilità più alta secondo un modello di Markov di ordine 2 ({max_markov_prob}): \"{max_markov_prob_sent}\".', )\n",
        "\n",
        "\n",
        "    def markov2(self, tokenized_sent):\n",
        "        '''Restituisce la probabilità di tokenized_sent secondo un modello di markov di ordine 2\n",
        "        (con add-1 smoothing) costruito a partire dal corpus di input.'''\n",
        "\n",
        "        len_voc = len(list(set(self.tokens)))\n",
        "        prob = (self.freq_dist[tokenized_sent[0]] + 1)/ (len(self.tokens) + len_voc) # P(A_1)\n",
        "        prob *= self.cond_prob((tokenized_sent[0], tokenized_sent[1])) # P(A_2|A_1)\n",
        "\n",
        "        sent_trigrams = list(ngrams(tokenized_sent, 3))\n",
        "        for trigram in sent_trigrams:\n",
        "            prob *= self.cond_prob(trigram) # P(A_n|A_{n-1}, A_{n-2})\n",
        "        return prob\n",
        "\n",
        "    def cond_prob(self, n_gram):\n",
        "        '''Dato un n-gramma (A_1, ..., A_n), restituisce la probabilità condizionata\n",
        "        P(A_n | A_1, ..., A_{n - 1}) calcolata con add-1 smoothing.'''\n",
        "\n",
        "        n = len(n_gram)\n",
        "        n_grams = list(ngrams(self.tokens, n))\n",
        "        prefix = tuple(n_gram[:-1]) # contesto\n",
        "        n_minus_one_grams = list(ngrams(self.tokens, n - 1))\n",
        "        prefix_freq = n_minus_one_grams.count(prefix) # frequenza del contesto\n",
        "        len_voc = len(list(set(self.tokens)))\n",
        "        return (n_grams.count(n_gram) + 1) / (prefix_freq + len_voc) # (F(A_1, .., A_n) + 1) / (F(A_1, ..., A_{n-1}) + |V|)\n",
        "\n",
        "    def ner(self, k):\n",
        "        '''Estrae le NE dal testo e mostra, per ciascuna classe, i k token più frequenti\n",
        "        (con relativa frequenza) ordinati per frequenza decrescente.'''\n",
        "\n",
        "        ne_tree = nltk.ne_chunk(nltk.tag.pos_tag(self.tokens)) # per NER uso le POS di Penn Treebank\n",
        "        ne = dict() # dizionario della forma {entity_type:[entities]}\n",
        "        for node in ne_tree: # accedo a ciascun nodo dell'albero\n",
        "            if hasattr(node, 'label'): # se il nodo corrisponde ad una NE, accedo alla classe di NE e alla NE\n",
        "                    entity_type = node.label()\n",
        "                    entity = \" \".join([token for token, pos in node.leaves()])\n",
        "                    if entity_type in ne.keys():\n",
        "                        ne[entity_type].append(entity)\n",
        "                    else:\n",
        "                        ne[entity_type] = [entity]\n",
        "\n",
        "        for entity_type, entities in ne.items():\n",
        "            print(entity_type)\n",
        "            freq_dist = nltk.FreqDist(entities)\n",
        "            top_k = {entity : freq_dist[entity] for entity in list(freq_dist)[:k]} # considero solo i primi k elementi\n",
        "            for i, entity in enumerate(top_k):\n",
        "                print(f'{i+1}. {entity} : {top_k[entity]}')\n",
        "            print('')\n",
        "\n",
        "    @staticmethod\n",
        "    def read_file(path_to_file):\n",
        "        '''Apre un file e lo restituisce come stringa.'''\n",
        "\n",
        "        with open (path_to_file, 'r', encoding='utf-8') as infile:\n",
        "            return infile.read()"
      ],
      "metadata": {
        "id": "743eEv0Pjo0v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze = Analyze('animal_farm.txt')"
      ],
      "metadata": {
        "id": "XHIGMBWcptAC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_words(50, pos_tags=['NOUN']) # top 50 sostantivi più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLl5hgrz3DwR",
        "outputId": "5609b54f-bfc3-4b7d-ec98-f255f9673cba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. animals : 243\n",
            "2. Napoleon : 165\n",
            "3. Snowball : 126\n",
            "4. farm : 109\n",
            "5. Boxer : 92\n",
            "6. pigs : 91\n",
            "7. Jones : 80\n",
            "8. time : 76\n",
            "9. windmill : 69\n",
            "10. Squealer : 66\n",
            "11. Farm : 55\n",
            "12. work : 53\n",
            "13. dogs : 50\n",
            "14. day : 47\n",
            "15. Animal : 46\n",
            "16. Mr. : 44\n",
            "17. animal : 44\n",
            "18. Clover : 42\n",
            "19. moment : 42\n",
            "20. comrades : 39\n",
            "21. days : 38\n",
            "22. men : 35\n",
            "23. farmhouse : 34\n",
            "24. Frederick : 34\n",
            "25. way : 33\n",
            "26. legs : 33\n",
            "27. Benjamin : 32\n",
            "28. sheep : 29\n",
            "29. England : 28\n",
            "30. Comrade : 28\n",
            "31. Pilkington : 28\n",
            "32. round : 27\n",
            "33. end : 27\n",
            "34. night : 25\n",
            "35. side : 25\n",
            "36. yard : 25\n",
            "37. barn : 25\n",
            "38. others : 24\n",
            "39. year : 23\n",
            "40. beings : 23\n",
            "41. hens : 22\n",
            "42. life : 22\n",
            "43. pig : 21\n",
            "44. nothing : 21\n",
            "45. Rebellion : 21\n",
            "46. Mollie : 20\n",
            "47. week : 20\n",
            "48. horses : 19\n",
            "49. food : 19\n",
            "50. eyes : 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_words(50, pos_tags=['ADV']) # top 50 avverbi più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd95u9YhulPT",
        "outputId": "e990edb7-25b6-4c45-dc27-97daffd2e45f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. not : 183\n",
            "2. when : 99\n",
            "3. now : 78\n",
            "4. so : 46\n",
            "5. never : 46\n",
            "6. very : 43\n",
            "7. even : 41\n",
            "8. then : 38\n",
            "9. back : 38\n",
            "10. more : 38\n",
            "11. ever : 37\n",
            "12. too : 32\n",
            "13. again : 32\n",
            "14. only : 31\n",
            "15. always : 29\n",
            "16. just : 27\n",
            "17. there : 27\n",
            "18. where : 26\n",
            "19. down : 26\n",
            "20. once : 25\n",
            "21. still : 24\n",
            "22. also : 24\n",
            "23. away : 24\n",
            "24. later : 23\n",
            "25. almost : 23\n",
            "26. well : 22\n",
            "27. as : 20\n",
            "28. When : 19\n",
            "29. already : 18\n",
            "30. soon : 18\n",
            "31. together : 18\n",
            "32. Then : 18\n",
            "33. how : 17\n",
            "34. suddenly : 17\n",
            "35. up : 16\n",
            "36. enough : 16\n",
            "37. Even : 16\n",
            "38. yet : 14\n",
            "39. immediately : 13\n",
            "40. far : 12\n",
            "41. forward : 12\n",
            "42. however : 11\n",
            "43. much : 11\n",
            "44. most : 10\n",
            "45. quite : 9\n",
            "46. nearly : 9\n",
            "47. Only : 9\n",
            "48. before : 9\n",
            "49. sometimes : 9\n",
            "50. finally : 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_words(50, pos_tags=['ADJ']) # top 50 aggettivi più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4scjV2GulXo",
        "outputId": "ac69afeb-e023-49c0-b746-bcd69cc8cd0a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. other : 60\n",
            "2. own : 40\n",
            "3. human : 39\n",
            "4. old : 38\n",
            "5. more : 31\n",
            "6. good : 29\n",
            "7. long : 28\n",
            "8. little : 28\n",
            "9. few : 28\n",
            "10. great : 27\n",
            "11. such : 26\n",
            "12. last : 25\n",
            "13. first : 24\n",
            "14. same : 21\n",
            "15. better : 19\n",
            "16. whole : 18\n",
            "17. hard : 18\n",
            "18. many : 17\n",
            "19. young : 16\n",
            "20. certain : 16\n",
            "21. white : 15\n",
            "22. able : 15\n",
            "23. terrible : 15\n",
            "24. big : 14\n",
            "25. much : 14\n",
            "26. small : 13\n",
            "27. tremendous : 13\n",
            "28. usual : 13\n",
            "29. animal : 13\n",
            "30. full : 13\n",
            "31. necessary : 13\n",
            "32. bad : 13\n",
            "33. large : 12\n",
            "34. next : 11\n",
            "35. black : 11\n",
            "36. late : 11\n",
            "37. green : 11\n",
            "38. dead : 9\n",
            "39. past : 9\n",
            "40. right : 9\n",
            "41. single : 8\n",
            "42. early : 8\n",
            "43. secret : 8\n",
            "44. new : 8\n",
            "45. several : 8\n",
            "46. strange : 7\n",
            "47. different : 7\n",
            "48. short : 7\n",
            "49. free : 7\n",
            "50. happy : 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 1) # top 20 unigrammi di parole più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHEq5Dn50408",
        "outputId": "7a74df0d-ee40-4822-ad6a-8a5a156ac094"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('the',) : 2020\n",
            "2. (',',) : 1938\n",
            "3. ('.',) : 1461\n",
            "4. ('and',) : 903\n",
            "5. ('of',) : 898\n",
            "6. ('to',) : 803\n",
            "7. ('was',) : 633\n",
            "8. ('a',) : 584\n",
            "9. ('had',) : 528\n",
            "10. ('in',) : 499\n",
            "11. ('that',) : 437\n",
            "12. ('it',) : 308\n",
            "13. ('were',) : 290\n",
            "14. ('they',) : 277\n",
            "15. ('his',) : 260\n",
            "16. ('animals',) : 243\n",
            "17. ('he',) : 242\n",
            "18. ('for',) : 241\n",
            "19. ('on',) : 233\n",
            "20. ('with',) : 220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 2) # top 20 bigrammi di parole più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuZIuScTvJ-j",
        "outputId": "e05a0338-2a19-451d-f8af-79eea58497c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. (',', 'and') : 316\n",
            "2. ('of', 'the') : 257\n",
            "3. ('in', 'the') : 179\n",
            "4. ('.', 'The') : 177\n",
            "5. ('the', 'animals') : 135\n",
            "6. (',', 'the') : 134\n",
            "7. ('had', 'been') : 109\n",
            "8. ('to', 'the') : 107\n",
            "9. ('it', 'was') : 100\n",
            "10. ('and', 'the') : 89\n",
            "11. ('the', 'farm') : 84\n",
            "12. (',', 'but') : 83\n",
            "13. ('on', 'the') : 83\n",
            "14. ('.', 'It') : 79\n",
            "15. ('.', '``') : 76\n",
            "16. (',', 'he') : 71\n",
            "17. ('.', 'He') : 71\n",
            "18. ('to', 'be') : 67\n",
            "19. ('that', 'the') : 65\n",
            "20. ('.', 'They') : 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 3) # top 20 trigrammi di parole più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy6VvKsqvN-K",
        "outputId": "daf835fe-e35f-4c28-e196-1b24d4b2e784"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. (',', 'and', 'the') : 51\n",
            "2. ('.', 'It', 'was') : 49\n",
            "3. ('.', 'The', 'animals') : 39\n",
            "4. (',', 'it', 'was') : 31\n",
            "5. ('he', 'said', ',') : 30\n",
            "6. (',', 'he', 'said') : 26\n",
            "7. ('on', 'the', 'farm') : 24\n",
            "8. ('of', 'the', 'farm') : 24\n",
            "9. (',', 'comrades', ',') : 21\n",
            "10. ('.', 'They', 'were') : 19\n",
            "11. ('the', 'animals', 'were') : 18\n",
            "12. (\"'Beasts\", 'of', 'England') : 18\n",
            "13. ('of', 'England', \"'\") : 18\n",
            "14. ('of', 'the', 'animals') : 18\n",
            "15. ('there', 'was', 'a') : 17\n",
            "16. ('the', 'other', 'animals') : 17\n",
            "17. ('the', 'farm', ',') : 17\n",
            "18. ('that', 'they', 'had') : 16\n",
            "19. ('all', 'the', 'animals') : 16\n",
            "20. ('.', 'In', 'the') : 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 4) # top 20 quadrigrammi di parole più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQmw77DvvSja",
        "outputId": "7a113fc4-82f3-45ce-ad7f-19e759976de8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. (',', 'he', 'said', ',') : 23\n",
            "2. (\"'Beasts\", 'of', 'England', \"'\") : 18\n",
            "3. ('the', 'Battle', 'of', 'the') : 13\n",
            "4. ('Four', 'legs', 'good', ',') : 12\n",
            "5. ('legs', 'good', ',', 'two') : 12\n",
            "6. ('good', ',', 'two', 'legs') : 12\n",
            "7. ('Battle', 'of', 'the', 'Cowshed') : 12\n",
            "8. (',', \"''\", 'he', 'said') : 10\n",
            "9. ('.', 'The', 'animals', 'were') : 10\n",
            "10. ('from', 'side', 'to', 'side') : 9\n",
            "11. (',', 'and', 'it', 'was') : 9\n",
            "12. (',', 'two', 'legs', 'bad') : 9\n",
            "13. ('on', 'the', 'farm', ',') : 8\n",
            "14. ('Jones', 'and', 'his', 'men') : 7\n",
            "15. ('.', 'It', 'was', 'a') : 7\n",
            "16. ('of', \"'Beasts\", 'of', 'England') : 7\n",
            "17. ('the', 'farm', 'buildings', ',') : 7\n",
            "18. ('the', 'big', 'barn', ',') : 6\n",
            "19. ('the', 'farm', ',', 'and') : 6\n",
            "20. ('the', 'animals', 'on', 'the') : 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 5) # top 20 quinquigrammi di parole più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-FTAzeVvVyq",
        "outputId": "b5376bf4-4d5d-4b75-c02c-6b1613671f8d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('Four', 'legs', 'good', ',', 'two') : 12\n",
            "2. ('legs', 'good', ',', 'two', 'legs') : 12\n",
            "3. ('the', 'Battle', 'of', 'the', 'Cowshed') : 12\n",
            "4. ('good', ',', 'two', 'legs', 'bad') : 9\n",
            "5. ('of', \"'Beasts\", 'of', 'England', \"'\") : 7\n",
            "6. (',', \"''\", 'he', 'said', ',') : 6\n",
            "7. (\"''\", 'he', 'said', ',', '``') : 6\n",
            "8. ('``', 'I', 'will', 'work', 'harder') : 6\n",
            "9. ('``', 'Four', 'legs', 'good', ',') : 6\n",
            "10. ('back', 'to', 'the', 'farm', 'buildings') : 5\n",
            "11. (',', 'two', 'legs', 'bad', '!') : 5\n",
            "12. ('Battle', 'of', 'the', 'Cowshed', ',') : 5\n",
            "13. ('at', 'the', 'Battle', 'of', 'the') : 5\n",
            "14. ('from', 'side', 'to', 'side', ',') : 4\n",
            "15. ('of', 'the', 'big', 'barn', ',') : 4\n",
            "16. ('.', '``', 'Comrades', ',', \"''\") : 4\n",
            "17. (\"'Beasts\", 'of', 'England', \"'\", '.') : 4\n",
            "18. ('at', 'the', 'foot', 'of', 'the') : 4\n",
            "19. ('singing', 'of', \"'Beasts\", 'of', 'England') : 4\n",
            "20. (\"'Beasts\", 'of', 'England', \"'\", ',') : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 1, pos=True) # top 20 unigrammi di pos più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s0eDA8evbFb",
        "outputId": "aab76ffa-25c8-4485-af2b-ba3723e9b667"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('NOUN',) : 7179\n",
            "2. ('VERB',) : 6241\n",
            "3. ('.',) : 4184\n",
            "4. ('ADP',) : 3936\n",
            "5. ('DET',) : 3932\n",
            "6. ('PRON',) : 2474\n",
            "7. ('ADV',) : 1994\n",
            "8. ('ADJ',) : 1856\n",
            "9. ('CONJ',) : 1223\n",
            "10. ('PRT',) : 1183\n",
            "11. ('NUM',) : 239\n",
            "12. ('X',) : 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 2, pos=True) # top 20 bigrammi di pos più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqoiNo1nvpte",
        "outputId": "9b5d7493-bbeb-4f15-b982-eef8c6ea5b54"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('DET', 'NOUN') : 2498\n",
            "2. ('NOUN', '.') : 2312\n",
            "3. ('ADP', 'DET') : 1619\n",
            "4. ('NOUN', 'ADP') : 1525\n",
            "5. ('NOUN', 'VERB') : 1346\n",
            "6. ('VERB', 'VERB') : 1325\n",
            "7. ('PRON', 'VERB') : 1293\n",
            "8. ('ADJ', 'NOUN') : 1117\n",
            "9. ('VERB', 'ADP') : 966\n",
            "10. ('ADP', 'NOUN') : 943\n",
            "11. ('VERB', 'ADV') : 828\n",
            "12. ('DET', 'ADJ') : 718\n",
            "13. ('VERB', 'DET') : 709\n",
            "14. ('.', 'PRON') : 673\n",
            "15. ('ADP', 'PRON') : 668\n",
            "16. ('.', 'DET') : 647\n",
            "17. ('.', 'CONJ') : 608\n",
            "18. ('.', 'NOUN') : 558\n",
            "19. ('VERB', 'PRON') : 552\n",
            "20. ('VERB', '.') : 543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 3, pos=True) # top 20 trigrammi di pos più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1DjedQTvve7",
        "outputId": "99b78174-22bb-405c-b060-a2f924daa54e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('ADP', 'DET', 'NOUN') : 1153\n",
            "2. ('DET', 'NOUN', 'ADP') : 676\n",
            "3. ('DET', 'NOUN', '.') : 648\n",
            "4. ('NOUN', 'ADP', 'DET') : 608\n",
            "5. ('DET', 'ADJ', 'NOUN') : 599\n",
            "6. ('.', 'PRON', 'VERB') : 580\n",
            "7. ('DET', 'NOUN', 'VERB') : 543\n",
            "8. ('NOUN', 'ADP', 'NOUN') : 487\n",
            "9. ('NOUN', 'VERB', 'VERB') : 439\n",
            "10. ('VERB', 'DET', 'NOUN') : 424\n",
            "11. ('ADJ', 'NOUN', '.') : 419\n",
            "12. ('PRON', 'VERB', 'VERB') : 410\n",
            "13. ('NOUN', '.', 'DET') : 386\n",
            "14. ('VERB', 'ADP', 'DET') : 384\n",
            "15. ('NOUN', '.', 'CONJ') : 365\n",
            "16. ('.', 'DET', 'NOUN') : 353\n",
            "17. ('NOUN', '.', 'PRON') : 344\n",
            "18. ('VERB', 'VERB', 'ADP') : 336\n",
            "19. ('NOUN', '.', 'NOUN') : 309\n",
            "20. ('ADP', 'DET', 'ADJ') : 307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_bigrams(10, pos_tags=('ADJ', 'NOUN')) # top 10 bigrammi (composti da aggettivo e sostantivo) per frequenza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdVGNixov6Ul",
        "outputId": "db02b4c6-e90a-4e5c-8517-f0edb39b7cbd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('human', 'beings') : 23\n",
            "2. ('other', 'animals') : 20\n",
            "3. ('big', 'barn') : 10\n",
            "4. ('first', 'time') : 8\n",
            "5. ('other', 'animal') : 7\n",
            "6. ('same', 'time') : 6\n",
            "7. ('old', 'Major') : 5\n",
            "8. ('few', 'minutes') : 5\n",
            "9. ('whole', 'farm') : 5\n",
            "10. ('next', 'moment') : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_bigrams(10, pos_tags=('ADJ', 'NOUN'), order='cond_prob') # top 10 bigrammi (composti da aggettivo e sostantivo) per probabiità condizionata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0GvmBe_wSEW",
        "outputId": "046254df-9c05-48e9-f28b-893cc661c6c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('handsome', 'money') : 1.0\n",
            "2. ('reasonable', 'price') : 1.0\n",
            "3. ('sordid', 'labour') : 1.0\n",
            "4. ('majestic-looking', 'pig') : 1.0\n",
            "5. ('admirable', 'care') : 1.0\n",
            "6. ('sick', 'comrade') : 1.0\n",
            "7. ('sidelong', 'look') : 1.0\n",
            "8. ('false', 'step') : 1.0\n",
            "9. ('turnip', 'field') : 1.0\n",
            "10. ('individual', 'letters') : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_bigrams(10, pos_tags=('ADJ', 'NOUN'), order='joint_prob') # top 10 bigrammi (composti da aggettivo e sostantivo) per probabiità congiunta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMCqoq7owiWK",
        "outputId": "6192db0b-8398-4d39-e38a-2bdc0ca8b322"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('human', 'beings') : 0.000667576118189998\n",
            "2. ('other', 'animals') : 0.0005805009723391287\n",
            "3. ('big', 'barn') : 0.00029025048616956433\n",
            "4. ('first', 'time') : 0.00023220038893565146\n",
            "5. ('other', 'animal') : 0.00020317534031869504\n",
            "6. ('same', 'time') : 0.0001741502917017386\n",
            "7. ('young', 'pigs') : 0.00014512524308478217\n",
            "8. ('next', 'moment') : 0.00014512524308478217\n",
            "9. ('outside', 'world') : 0.00014512524308478217\n",
            "10. ('five-barred', 'gate') : 0.00014512524308478217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_bigrams(10, pos_tags=('ADJ', 'NOUN'), order='mi') # top 10 bigrammi (composti da aggettivo e sostantivo) per mutua informazione"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqAx_-OlwmYJ",
        "outputId": "687b6ba0-4fc8-4961-c1a3-13b1f78ce2f5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('false', 'step') : 20.01927051117619\n",
            "2. ('nesting', 'boxes') : 20.01927051117619\n",
            "3. ('swift', 'dash') : 20.01927051117619\n",
            "4. ('unforeseen', 'shortages') : 20.01927051117619\n",
            "5. ('momentary', 'awkwardness') : 20.01927051117619\n",
            "6. ('fourth', 'foal') : 20.01927051117619\n",
            "7. ('rash', 'actions') : 20.01927051117619\n",
            "8. ('Own', 'Bricklayer') : 20.01927051117619\n",
            "9. ('deepest', 'distress') : 20.01927051117619\n",
            "10. ('basic', 'slag') : 20.01927051117619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_bigrams(10, pos_tags=('ADJ', 'NOUN'), order='lmi') # top 10 bigrammi (composti da aggettivo e sostantivo) per mutua informazione locale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs4NV_x1wwJj",
        "outputId": "0ef68968-352a-40ee-cd12-583132319ee4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('human', 'beings') : 337.2195256508365\n",
            "2. ('other', 'animals') : 210.18991013698508\n",
            "3. ('big', 'barn') : 148.89987494231224\n",
            "4. ('first', 'time') : 94.17074398186084\n",
            "5. ('five-barred', 'gate') : 86.05957794559293\n",
            "6. ('outside', 'world') : 79.16701982932429\n",
            "7. ('other', 'animal') : 77.60791376422374\n",
            "8. ('same', 'time') : 71.78392845405\n",
            "9. ('few', 'minutes') : 70.37206032684325\n",
            "10. ('next', 'moment') : 67.44724782323749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.common_mi_lmi(10, ('ADJ', 'NOUN'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6w5oZF-wy_N",
        "outputId": "c7a3ca57-cee6-482a-af1d-a620b97e1139"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non ci sono bigrammi in comune nei primi 10 bigrammi con pos ('ADJ', 'NOUN') ordinati per mutual information e local mutual information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.analyze_sentences()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U24rKASuw9CI",
        "outputId": "c65d1125-3cc7-4304-844b-1bbdef48102e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase con la media della distribuzione di frequenza dei token più alta (899.25): \"After the horses came Muriel,\n",
            "the white goat, and Benjamin, the donkey.\".\n",
            "Frase con la media della distribuzione di frequenza dei token più bassa (53.11): \"Others asked such questions as \"Why should we care what\n",
            "happens after we are dead?\"\".\n",
            "Frase con la probabilità più alta secondo un modello di Markov di ordine 2 (6.472902553197826e-30): \"It was a pig walking on his hind legs.\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.ner(15) # top 15 NE più frequenti per ciascuna classe di NE con relativa frequenza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_Ox6t6PxAWn",
        "outputId": "db6894c9-1c3a-4c50-ca5e-286826ed11ea"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERSON\n",
            "1. Napoleon : 112\n",
            "2. Boxer : 73\n",
            "3. Squealer : 65\n",
            "4. Snowball : 63\n",
            "5. Jones : 46\n",
            "6. Animal Farm : 37\n",
            "7. Benjamin : 31\n",
            "8. Clover : 26\n",
            "9. Frederick : 24\n",
            "10. Mr. Jones : 22\n",
            "11. Mollie : 20\n",
            "12. Muriel : 14\n",
            "13. Mr. Pilkington : 14\n",
            "14. Major : 10\n",
            "15. Foxwood : 9\n",
            "\n",
            "GPE\n",
            "1. England : 28\n",
            "2. Napoleon : 22\n",
            "3. Snowball : 18\n",
            "4. Boxer : 13\n",
            "5. Jones : 12\n",
            "6. Clover : 9\n",
            "7. Animalism : 8\n",
            "8. Pilkington : 7\n",
            "9. Willingdon : 5\n",
            "10. Long : 5\n",
            "11. Whymper : 5\n",
            "12. Frederick : 4\n",
            "13. Foxwood : 3\n",
            "14. Jessie : 2\n",
            "15. Ireland : 2\n",
            "\n",
            "ORGANIZATION\n",
            "1. Snowball : 27\n",
            "2. Comrade Napoleon : 24\n",
            "3. Rebellion : 16\n",
            "4. Battle : 13\n",
            "5. Seven Commandments : 10\n",
            "6. Cowshed : 8\n",
            "7. Bluebell : 3\n",
            "8. Republic : 3\n",
            "9. Willingdon : 3\n",
            "10. Sundays : 3\n",
            "11. Commandment : 3\n",
            "12. Berkshire : 2\n",
            "13. Red Lion : 2\n",
            "14. Animals : 2\n",
            "15. Second Class : 2\n",
            "\n",
            "LOCATION\n",
            "1. Clean Tails League : 1\n",
            "\n",
            "FACILITY\n",
            "1. Crown Derby : 2\n",
            "2. Fifth Commandment : 1\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Progetto di Linguistica Computazionale di Giacomo Fidone (531668) – Estrazione di informazioni da constitution_of_the_united_states.txt"
      ],
      "metadata": {
        "id": "Y7MnArkbjJ9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "_CUl_SYni1Ma"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eEmFlbuQil6Z"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import ngrams\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvH4m9zSi4Jv",
        "outputId": "8ae26270-49c0-4a1e-f6f1-2445bc21c6cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estrazione di Informazioni da un testo"
      ],
      "metadata": {
        "id": "9EjyA0o_jmGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Analyze:\n",
        "\n",
        "    def __init__(self, path):\n",
        "        '''Definisce una classe per l'estrazione di informazioni da un testo.'''\n",
        "\n",
        "        self.text = self.read_file(path)\n",
        "        self.sents = nltk.tokenize.sent_tokenize(self.text)\n",
        "        self.tokenized_sents = [nltk.word_tokenize(sent) for sent in self.sents]\n",
        "        self.tokens = [token for sent in self.tokenized_sents for token in sent]\n",
        "        self.freq_dist = nltk.FreqDist(self.tokens)\n",
        "        self.pos = nltk.tag.pos_tag(self.tokens, tagset='universal') # uso il tagset di UD\n",
        "        self.only_pos = [pos for token, pos in self.pos]\n",
        "\n",
        "    def top_k_words(self, k, pos_tags=None):\n",
        "        '''Mostra i k token più frequenti (e relativa frequenza) in ordine decrescente. E' possibile indicare i pos-tags da\n",
        "        considerare assegnando a \"pos_tags\" la lista di UD pos tags. '''\n",
        "\n",
        "        if pos_tags==None:\n",
        "            freqs = self.freq_dist\n",
        "        else:\n",
        "            tokens = [token for token, pos in self.pos if pos in pos_tags]\n",
        "            freqs = nltk.FreqDist(tokens)\n",
        "\n",
        "        top_k = {token : freqs[token] for token in list(freqs)[:k]}\n",
        "        for i, token in enumerate(top_k):\n",
        "            print(f'{i+1}. {token} : {top_k[token]}')\n",
        "\n",
        "    def top_k_ngrams(self, k, n, pos=False):\n",
        "        '''Mostra i primi k n-grammi di parole più frequenti (e relative frequenze) in ordine decrescente. Se \"pos\"=True,\n",
        "        considera n-grammi di pos.'''\n",
        "\n",
        "        if pos:\n",
        "            n_grams = list(ngrams(self.only_pos, n))\n",
        "        else:\n",
        "            n_grams = list(ngrams(self.tokens, n))\n",
        "        voc = list(set(n_grams))\n",
        "        freqs = nltk.FreqDist(n_grams)\n",
        "\n",
        "        top_k = {n_gram : freqs[n_gram] for n_gram in list(freqs)[:k]}\n",
        "        for i, n_gram in enumerate(top_k):\n",
        "            print(f'{i+1}. {n_gram} : {top_k[n_gram]}')\n",
        "\n",
        "    def top_k_bigrams(self, k, pos_tags, order='frequency', display_output=True):\n",
        "        '''Mostra i primi k bigrammi di parole, le cui pos corrispondono a \"pos_tags\", in base a \"order\". Valori possibili di \"order\" sono: \"frequency\" (ordinati per frequenza – default);\n",
        "        \"cond_prob\" (ordinati per probabilità condizionata), \"joint_prob\" (ordinati per probabilità congiunta), \"mi\" (ordinati per mutual information),\n",
        "        \"lmi\" (ordinati per local mutual information). Il metodo è usato internamente da self.common_mi_lmi(), che disattiva il flag \"display_output\".'''\n",
        "\n",
        "        bigrams = list(ngrams(self.tokens, 2))\n",
        "        pos_bigrams = list(ngrams(self.only_pos, 2))\n",
        "        bigrams = [bigram for bigram, pos_bigram in zip(bigrams, pos_bigrams) if pos_bigram==pos_tags] # seleziono i bigrammi le cui pos corrispodono a \"pos_tags\"\n",
        "        bigrams_freq_dist = nltk.FreqDist(bigrams)\n",
        "\n",
        "        results = dict() # dizionario della forma {bigramma:valore}, dove valore è il valore della misura selezionata per \"order\"\n",
        "        if order == 'frequency':\n",
        "            results = bigrams_freq_dist\n",
        "        else:\n",
        "            voc = list(set(bigrams)) # vocabolario di bigrammi\n",
        "            for bigram in voc:\n",
        "                bigram_freq = bigrams_freq_dist[bigram]\n",
        "                first_token_freq = self.freq_dist[bigram[0]]\n",
        "                corpus_size = len(self.tokens)\n",
        "                if order == 'cond_prob' or order=='joint_prob':\n",
        "                    cond_prob = bigram_freq / first_token_freq # P(B|A) = F(A, B) / F(A)\n",
        "                    if order == 'cond_prob':\n",
        "                        results[bigram] = cond_prob\n",
        "                    else:\n",
        "                        results[bigram] = (first_token_freq/corpus_size) * cond_prob # P(A, B) = P(A)P(B|A)\n",
        "                else:\n",
        "                    first_token_prob = first_token_freq / corpus_size\n",
        "                    second_token_prob = self.freq_dist[bigram[1]] / corpus_size\n",
        "                    bigram_prob = bigram_freq / len(bigrams)\n",
        "                    mi = math.log(bigram_prob / (first_token_prob * second_token_prob), 2) # MI = log_2(P(A, B) / P(A) * P(B))\n",
        "                    if order == 'mi':\n",
        "                        results[bigram] = mi\n",
        "                    elif order == 'lmi':\n",
        "                        results[bigram] = bigram_freq * mi # LMI = F(A, B) * MI\n",
        "                    else:\n",
        "                        raise ValueError(f'Invalid value for \"order\". Expected one of [\"frequency\", \"cond_prob\", \"joint_prob\", \"mi\", \"lmi\"], got \"{order}\".')\n",
        "            results = dict(sorted(results.items(), key=lambda item: item[1], reverse=True)) # ordino i bigrammi in senso decrescente secondo il valore assegnato\n",
        "\n",
        "        top_k = {bigram : results[bigram] for bigram in list(results)[:k]} # considero solo i primi k bigrammi\n",
        "        if display_output:\n",
        "            for i, bigram in enumerate(top_k):\n",
        "                print(f'{i+1}. {bigram} : {top_k[bigram]}')\n",
        "        else:\n",
        "            return top_k.keys()\n",
        "\n",
        "    def common_mi_lmi(self, k, pos_tags, display_output=False):\n",
        "        '''Mostra i bigrammi in comune ai primi k bigrammi (le cui pos corrispondono a 'pos_tags') ordinati per mutual information e local mutual information.'''\n",
        "\n",
        "        mi_bigrams = self.top_k_bigrams(k, pos_tags, order='mi', display_output=False)\n",
        "        lmi_bigrams = self.top_k_bigrams(k, pos_tags, order='lmi', display_output=False)\n",
        "        common = [bigram for bigram in mi_bigrams if bigram in lmi_bigrams]\n",
        "        if common:\n",
        "            print(f'Bigrammi in comune nei primi {k} bigrammi con pos {pos_tags} ordinati per mutual information e local mutual information:')\n",
        "            for bigram in common:\n",
        "                print(bigram)\n",
        "        else:\n",
        "            print(f'Non ci sono bigrammi in comune nei primi {k} bigrammi con pos {pos_tags} ordinati per mutual information e local mutual information.')\n",
        "\n",
        "    def analyze_sentences(self):\n",
        "        '''Mostra la frase con la media della distribuzione di frequenza massima, la frase con la media di distribuzione di frequenza minima\n",
        "        e la frase con la più alta probabilità secondo un modello di markov di ordine 2. Considera solo frasi la cui lunghezza è compresa tra 10 e 20 token\n",
        "        e di cui almeno metà dei token non è un hapax.'''\n",
        "\n",
        "        target_sents = [sent for sent in self.tokenized_sents if len(sent) <=20 and len(sent) >= 10] # considero solo frasi la cui lunghezza è compresa tra 10 e 20\n",
        "        for sent in target_sents:\n",
        "            no_hapax_words = len([token for token in sent if self.freq_dist[token] > 1]) # calcolo il numero di parole della frase con frequenza maggiore di 1\n",
        "            if no_hapax_words < (len(sent) // 2): # rimuovo la frase se il numero di parole che non sono hapax è inferiore alla metà del numero di parole totale\n",
        "                target_sents.remove(sent)\n",
        "\n",
        "        max_avg_freq = 0\n",
        "        max_avg_freq_sent = str()\n",
        "        min_avg_freq = float('inf')\n",
        "        min_avg_freq_sent = str()\n",
        "        max_markov_prob = 0\n",
        "        max_markov_prob_sent = str()\n",
        "\n",
        "        for sent in target_sents:\n",
        "            token_freqs = [self.freq_dist[token] for token in sent]\n",
        "            avg_freq = sum(token_freqs) / len(sent) # media della distribuzione di frequenza\n",
        "            markov_prob = self.markov2(sent) # probabilità secondo modello di markov di ordine 2\n",
        "            if avg_freq > max_avg_freq:\n",
        "                max_avg_freq = avg_freq\n",
        "                max_avg_freq_sent = sent\n",
        "            if avg_freq < min_avg_freq:\n",
        "                min_avg_freq = avg_freq\n",
        "                min_avg_freq_sent = sent\n",
        "            if markov_prob > max_markov_prob:\n",
        "                max_markov_prob = markov_prob\n",
        "                max_markov_prob_sent = sent\n",
        "\n",
        "        max_avg_freq_sent = self.sents[self.tokenized_sents.index(max_avg_freq_sent)] # recupero le frasi non tokenizzate per la stampa a video\n",
        "        min_avg_freq_sent = self.sents[self.tokenized_sents.index(min_avg_freq_sent)]\n",
        "        max_markov_prob_sent = self.sents[self.tokenized_sents.index(max_markov_prob_sent)]\n",
        "\n",
        "        print(f'Frase con la media della distribuzione di frequenza dei token più alta ({round(max_avg_freq, 2)}): \"{max_avg_freq_sent}\".', )\n",
        "        print(f'Frase con la media della distribuzione di frequenza dei token più bassa ({round(min_avg_freq, 2)}): \"{min_avg_freq_sent}\".', )\n",
        "        print(f'Frase con la probabilità più alta secondo un modello di Markov di ordine 2 ({max_markov_prob}): \"{max_markov_prob_sent}\".', )\n",
        "\n",
        "\n",
        "    def markov2(self, tokenized_sent):\n",
        "        '''Restituisce la probabilità di tokenized_sent secondo un modello di markov di ordine 2\n",
        "        (con add-1 smoothing) costruito a partire dal corpus di input.'''\n",
        "\n",
        "        len_voc = len(list(set(self.tokens)))\n",
        "        prob = (self.freq_dist[tokenized_sent[0]] + 1)/ (len(self.tokens) + len_voc) # P(A_1)\n",
        "        prob *= self.cond_prob((tokenized_sent[0], tokenized_sent[1])) # P(A_2|A_1)\n",
        "\n",
        "        sent_trigrams = list(ngrams(tokenized_sent, 3))\n",
        "        for trigram in sent_trigrams:\n",
        "            prob *= self.cond_prob(trigram) # P(A_n|A_{n-1}, A_{n-2})\n",
        "        return prob\n",
        "\n",
        "    def cond_prob(self, n_gram):\n",
        "        '''Dato un n-gramma (A_1, ..., A_n), restituisce la probabilità condizionata\n",
        "        P(A_n | A_1, ..., A_{n - 1}) calcolata con add-1 smoothing.'''\n",
        "\n",
        "        n = len(n_gram)\n",
        "        n_grams = list(ngrams(self.tokens, n))\n",
        "        prefix = tuple(n_gram[:-1]) # contesto\n",
        "        n_minus_one_grams = list(ngrams(self.tokens, n - 1))\n",
        "        prefix_freq = n_minus_one_grams.count(prefix) # frequenza del contesto\n",
        "        len_voc = len(list(set(self.tokens)))\n",
        "        return (n_grams.count(n_gram) + 1) / (prefix_freq + len_voc) # (F(A_1, .., A_n) + 1) / (F(A_1, ..., A_{n-1}) + |V|)\n",
        "\n",
        "    def ner(self, k):\n",
        "        '''Estrae le NE dal testo e mostra, per ciascuna classe, i k token più frequenti\n",
        "        (con relativa frequenza) ordinati per frequenza decrescente.'''\n",
        "\n",
        "        ne_tree = nltk.ne_chunk(nltk.tag.pos_tag(self.tokens)) # per NER uso le POS di Penn Treebank\n",
        "        ne = dict() # dizionario della forma {entity_type:[entities]}\n",
        "        for node in ne_tree: # accedo a ciascun nodo dell'albero\n",
        "            if hasattr(node, 'label'): # se il nodo corrisponde ad una NE, accedo alla classe di NE e alla NE\n",
        "                    entity_type = node.label()\n",
        "                    entity = \" \".join([token for token, pos in node.leaves()])\n",
        "                    if entity_type in ne.keys():\n",
        "                        ne[entity_type].append(entity)\n",
        "                    else:\n",
        "                        ne[entity_type] = [entity]\n",
        "\n",
        "        for entity_type, entities in ne.items():\n",
        "            print(entity_type)\n",
        "            freq_dist = nltk.FreqDist(entities)\n",
        "            top_k = {entity : freq_dist[entity] for entity in list(freq_dist)[:k]} # considero solo i primi k elementi\n",
        "            for i, entity in enumerate(top_k):\n",
        "                print(f'{i+1}. {entity} : {top_k[entity]}')\n",
        "            print('')\n",
        "\n",
        "    @staticmethod\n",
        "    def read_file(path_to_file):\n",
        "        '''Apre un file e lo restituisce come stringa.'''\n",
        "\n",
        "        with open (path_to_file, 'r', encoding='utf-8') as infile:\n",
        "            return infile.read()"
      ],
      "metadata": {
        "id": "743eEv0Pjo0v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze = Analyze('constitution_of_the_united_states.txt')"
      ],
      "metadata": {
        "id": "XHIGMBWcptAC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_words(50, pos_tags=['NOUN']) # top 50 sostantivi più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLl5hgrz3DwR",
        "outputId": "d5a0e4fb-933a-4985-b200-901c07bdfdc3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. States : 221\n",
            "2. February : 162\n",
            "3. January : 160\n",
            "4. March : 141\n",
            "5. United : 120\n",
            "6. President : 118\n",
            "7. Congress : 116\n",
            "8. State : 103\n",
            "9. amendment : 103\n",
            "10. * : 91\n",
            "11. New : 80\n",
            "12. April : 69\n",
            "13. Constitution : 68\n",
            "14. June : 62\n",
            "15. Section : 59\n",
            "16. May : 54\n",
            "17. House : 49\n",
            "18. December : 49\n",
            "19. Senate : 44\n",
            "20. July : 44\n",
            "21. Article : 43\n",
            "22. Representatives : 39\n",
            "23. Carolina : 39\n",
            "24. legislatures : 39\n",
            "25. Ratification : 38\n",
            "26. Virginia : 38\n",
            "27. ratification : 34\n",
            "28. article : 31\n",
            "29. North : 30\n",
            "30. Vice : 29\n",
            "31. South : 28\n",
            "32. York : 24\n",
            "33. [ : 24\n",
            "34. September : 23\n",
            "35. Massachusetts : 23\n",
            "36. Hampshire : 23\n",
            "37. Law : 23\n",
            "38. August : 23\n",
            "39. Jersey : 22\n",
            "40. Connecticut : 22\n",
            "41. Maryland : 22\n",
            "42. time : 22\n",
            "43. Delaware : 21\n",
            "44. Pennsylvania : 21\n",
            "45. November : 21\n",
            "46. Rhode : 20\n",
            "47. Island : 20\n",
            "48. office : 20\n",
            "49. Vermont : 19\n",
            "50. Office : 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_words(50, pos_tags=['ADV']) # top 50 avverbi più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd95u9YhulPT",
        "outputId": "e065d54f-288f-4777-93e1-1714d961a3f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. not : 67\n",
            "2. subsequently : 29\n",
            "3. when : 25\n",
            "4. then : 13\n",
            "5. so : 7\n",
            "6. previously : 7\n",
            "7. otherwise : 5\n",
            "8. only : 4\n",
            "9. respectively : 4\n",
            "10. hereby : 4\n",
            "11. as : 3\n",
            "12. When : 3\n",
            "13. once : 3\n",
            "14. immediately : 3\n",
            "15. whenever : 3\n",
            "16. Whenever : 3\n",
            "17. how : 2\n",
            "18. effectually : 2\n",
            "19. most : 2\n",
            "20. equally : 2\n",
            "21. also : 2\n",
            "22. together : 2\n",
            "23. prior : 2\n",
            "24. before : 2\n",
            "25. accordingly : 2\n",
            "26. faithfully : 2\n",
            "27. where : 2\n",
            "28. well : 2\n",
            "29. twice : 2\n",
            "30. more : 1\n",
            "31. ordain : 1\n",
            "32. far : 1\n",
            "33. essentially : 1\n",
            "34. already : 1\n",
            "35. promptly : 1\n",
            "36. unanimously : 1\n",
            "37. \\2\\Immediately : 1\n",
            "38. nevertheless : 1\n",
            "39. likewise : 1\n",
            "40. square : 1\n",
            "41. now : 1\n",
            "42. absolutely : 1\n",
            "43. enter : 1\n",
            "44. actually : 1\n",
            "45. neither : 1\n",
            "46. solemnly : 1\n",
            "47. alone : 1\n",
            "48. thereof : 1\n",
            "49. thereby : 1\n",
            "50. ever : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_words(50, pos_tags=['ADJ']) # top 50 aggettivi più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4scjV2GulXo",
        "outputId": "9238fc3b-dbbd-4616-d94e-a280220622b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. such : 56\n",
            "2. several : 42\n",
            "3. other : 37\n",
            "4. * : 26\n",
            "5. same : 15\n",
            "6. following : 14\n",
            "7. first : 14\n",
            "8. valid : 14\n",
            "9. necessary : 12\n",
            "10. public : 12\n",
            "11. whole : 10\n",
            "12. more : 10\n",
            "13. appropriate : 9\n",
            "14. second : 8\n",
            "15. respective : 7\n",
            "16. foreign : 7\n",
            "17. ] : 7\n",
            "18. \\1\\The : 6\n",
            "19. equal : 6\n",
            "20. common : 5\n",
            "21. least : 5\n",
            "22. ten : 5\n",
            "23. supreme : 5\n",
            "24. judicial : 5\n",
            "25. inoperative : 5\n",
            "26. original : 4\n",
            "27. uniform : 4\n",
            "28. present : 4\n",
            "29. new : 4\n",
            "30. subject : 4\n",
            "31. different : 4\n",
            "32. greatest : 4\n",
            "33. unable : 4\n",
            "34. domestic : 3\n",
            "35. general : 3\n",
            "36. free : 3\n",
            "37. actual : 3\n",
            "38. subsequent : 3\n",
            "39. fourth : 3\n",
            "40. next : 3\n",
            "41. civil : 3\n",
            "42. executive : 3\n",
            "43. highest : 3\n",
            "44. eligible : 3\n",
            "45. principal : 3\n",
            "46. inferior : 3\n",
            "47. male : 3\n",
            "48. twenty-one : 3\n",
            "49. three- : 3\n",
            "50. less : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 1) # top 20 unigrammi di parole più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHEq5Dn50408",
        "outputId": "ea64e02f-7040-49ce-840f-0954f955e3c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. (',',) : 2311\n",
            "2. ('--',) : 1217\n",
            "3. ('the',) : 1092\n",
            "4. ('of',) : 816\n",
            "5. (';',) : 734\n",
            "6. ('.',) : 440\n",
            "7. ('and',) : 372\n",
            "8. ('shall',) : 333\n",
            "9. ('to',) : 314\n",
            "10. ('by',) : 262\n",
            "11. ('States',) : 221\n",
            "12. ('be',) : 210\n",
            "13. ('in',) : 181\n",
            "14. ('on',) : 173\n",
            "15. ('or',) : 173\n",
            "16. ('February',) : 162\n",
            "17. ('January',) : 160\n",
            "18. ('March',) : 141\n",
            "19. ('a',) : 131\n",
            "20. ('*',) : 130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 2) # top 20 bigrammi di parole più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuZIuScTvJ-j",
        "outputId": "0f512e7b-3879-4025-abbc-1000ba0196c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('--', '--') : 1164\n",
            "2. ('of', 'the') : 341\n",
            "3. (',', 'January') : 139\n",
            "4. (',', 'and') : 135\n",
            "5. (',', 'February') : 133\n",
            "6. ('the', 'United') : 119\n",
            "7. ('United', 'States') : 119\n",
            "8. ('shall', 'be') : 119\n",
            "9. ('by', 'the') : 119\n",
            "10. (',', 'March') : 116\n",
            "11. ('to', 'the') : 115\n",
            "12. ('*', '*') : 104\n",
            "13. ('.', 'The') : 86\n",
            "14. (',', '1933') : 74\n",
            "15. (',', 'or') : 71\n",
            "16. ('States', ',') : 69\n",
            "17. (';', 'New') : 63\n",
            "18. (',', 'April') : 63\n",
            "19. ('1933', ';') : 63\n",
            "20. (',', '1919') : 57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 3) # top 20 trigrammi di parole più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy6VvKsqvN-K",
        "outputId": "c26c5858-e8d3-45be-d538-96fcb4ce9d5d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('--', '--', '--') : 1127\n",
            "2. ('the', 'United', 'States') : 118\n",
            "3. ('of', 'the', 'United') : 89\n",
            "4. ('*', '*', '*') : 78\n",
            "5. (',', '1933', ';') : 63\n",
            "6. (',', '1919', ';') : 49\n",
            "7. ('United', 'States', ',') : 41\n",
            "8. (',', '1971', ';') : 41\n",
            "9. ('the', 'several', 'States') : 36\n",
            "10. ('the', 'legislatures', 'of') : 35\n",
            "11. (',', '1913', ';') : 34\n",
            "12. (',', '1961', ';') : 34\n",
            "13. (',', '1963', ';') : 34\n",
            "14. ('--', '--', '-') : 32\n",
            "15. ('to', 'the', 'Constitution') : 31\n",
            "16. ('ratified', 'by', 'the') : 28\n",
            "17. ('of', 'the', 'several') : 28\n",
            "18. (',', '1865', ';') : 28\n",
            "19. ('(', 'after', 'having') : 28\n",
            "20. ('after', 'having', 'rejected') : 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 4) # top 20 quadrigrammi di parole più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQmw77DvvSja",
        "outputId": "a5ddf98a-0b95-4b68-c360-04f9b331ef9d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('--', '--', '--', '--') : 1090\n",
            "2. ('of', 'the', 'United', 'States') : 88\n",
            "3. ('*', '*', '*', '*') : 52\n",
            "4. ('the', 'United', 'States', ',') : 41\n",
            "5. ('--', '--', '--', '-') : 32\n",
            "6. ('(', 'after', 'having', 'rejected') : 28\n",
            "7. ('.', '--', '--', '--') : 27\n",
            "8. ('.', 'The', 'amendment', 'was') : 27\n",
            "9. ('amendment', 'to', 'the', 'Constitution') : 26\n",
            "10. ('after', 'having', 'rejected', 'it') : 26\n",
            "11. ('of', 'the', 'several', 'States') : 25\n",
            "12. ('having', 'rejected', 'it', 'on') : 25\n",
            "13. ('by', 'the', 'legislatures', 'of') : 22\n",
            "14. ('the', 'Constitution', 'of', 'the') : 21\n",
            "15. ('Constitution', 'of', 'the', 'United') : 20\n",
            "16. ('was', 'subsequently', 'ratified', 'by') : 19\n",
            "17. (';', 'New', 'Hampshire', ',') : 19\n",
            "18. ('Ratification', 'was', 'completed', 'on') : 19\n",
            "19. ('ratified', 'by', 'the', 'legislatures') : 18\n",
            "20. ('have', 'been', 'ratified', 'by') : 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 5) # top 20 quinquigrammi di parole più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-FTAzeVvVyq",
        "outputId": "678227f3-4057-4fe5-a387-dc55079ea8b5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('--', '--', '--', '--', '--') : 1057\n",
            "2. ('of', 'the', 'United', 'States', ',') : 32\n",
            "3. ('--', '--', '--', '--', '-') : 32\n",
            "4. ('.', '--', '--', '--', '--') : 27\n",
            "5. ('*', '*', '*', '*', '*') : 26\n",
            "6. ('(', 'after', 'having', 'rejected', 'it') : 26\n",
            "7. ('after', 'having', 'rejected', 'it', 'on') : 25\n",
            "8. ('the', 'Constitution', 'of', 'the', 'United') : 20\n",
            "9. ('Constitution', 'of', 'the', 'United', 'States') : 20\n",
            "10. ('ratified', 'by', 'the', 'legislatures', 'of') : 18\n",
            "11. ('to', 'the', 'Constitution', 'of', 'the') : 17\n",
            "12. ('to', 'have', 'been', 'ratified', 'by') : 17\n",
            "13. ('.', 'Ratification', 'was', 'completed', 'on') : 17\n",
            "14. ('.', 'The', 'amendment', 'was', 'subsequently') : 17\n",
            "15. ('The', 'amendment', 'was', 'subsequently', 'ratified') : 17\n",
            "16. ('amendment', 'was', 'subsequently', 'ratified', 'by') : 17\n",
            "17. ('--', '--', '--', '-', '*') : 16\n",
            "18. ('--', '--', '-', '*', '*') : 16\n",
            "19. ('--', '-', '*', '*', '*') : 16\n",
            "20. ('-', '*', '*', '*', '*') : 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 1, pos=True) # top 20 unigrammi di pos più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s0eDA8evbFb",
        "outputId": "73c6061f-012b-4746-8ff2-f034adf988f4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('NOUN',) : 5489\n",
            "2. ('.',) : 4933\n",
            "3. ('VERB',) : 2096\n",
            "4. ('ADP',) : 2048\n",
            "5. ('NUM',) : 1885\n",
            "6. ('DET',) : 1768\n",
            "7. ('CONJ',) : 619\n",
            "8. ('ADJ',) : 612\n",
            "9. ('PRT',) : 321\n",
            "10. ('PRON',) : 258\n",
            "11. ('ADV',) : 245\n",
            "12. ('X',) : 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 2, pos=True) # top 20 bigrammi di pos più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqoiNo1nvpte",
        "outputId": "bef4fad5-9476-432d-bbcd-be8a766c4317"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('NUM', '.') : 1703\n",
            "2. ('.', 'NOUN') : 1633\n",
            "3. ('NOUN', '.') : 1630\n",
            "4. ('DET', 'NOUN') : 1336\n",
            "5. ('.', '.') : 1307\n",
            "6. ('NOUN', 'ADP') : 1120\n",
            "7. ('ADP', 'DET') : 972\n",
            "8. ('NOUN', 'NUM') : 885\n",
            "9. ('.', 'NUM') : 852\n",
            "10. ('NOUN', 'NOUN') : 798\n",
            "11. ('VERB', 'VERB') : 723\n",
            "12. ('ADP', 'NOUN') : 676\n",
            "13. ('NOUN', 'VERB') : 540\n",
            "14. ('ADJ', 'NOUN') : 463\n",
            "15. ('VERB', 'ADP') : 399\n",
            "16. ('.', 'CONJ') : 324\n",
            "17. ('.', 'ADP') : 259\n",
            "18. ('DET', 'ADJ') : 240\n",
            "19. ('VERB', 'DET') : 234\n",
            "20. ('NOUN', 'CONJ') : 231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_ngrams(20, 3, pos=True) # top 20 trigrammi di pos più frequenti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1DjedQTvve7",
        "outputId": "d7333f5e-24a6-4278-8148-fc7910877c46"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('.', '.', '.') : 1199\n",
            "2. ('NOUN', 'NUM', '.') : 865\n",
            "3. ('NOUN', '.', 'NOUN') : 851\n",
            "4. ('.', 'NUM', '.') : 832\n",
            "5. ('NUM', '.', 'NUM') : 796\n",
            "6. ('ADP', 'DET', 'NOUN') : 732\n",
            "7. ('.', 'NOUN', 'NUM') : 725\n",
            "8. ('NUM', '.', 'NOUN') : 664\n",
            "9. ('.', 'NOUN', '.') : 569\n",
            "10. ('NOUN', 'ADP', 'DET') : 562\n",
            "11. ('DET', 'NOUN', 'ADP') : 519\n",
            "12. ('NOUN', 'ADP', 'NOUN') : 398\n",
            "13. ('NOUN', 'NOUN', '.') : 358\n",
            "14. ('NOUN', 'VERB', 'VERB') : 291\n",
            "15. ('DET', 'NOUN', 'VERB') : 227\n",
            "16. ('NOUN', '.', 'CONJ') : 225\n",
            "17. ('.', 'NOUN', 'NOUN') : 225\n",
            "18. ('DET', 'NOUN', 'NOUN') : 223\n",
            "19. ('VERB', 'VERB', 'ADP') : 216\n",
            "20. ('DET', 'ADJ', 'NOUN') : 209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_bigrams(10, pos_tags=('ADJ', 'NOUN')) # top 10 bigrammi (composti da aggettivo e sostantivo) per frequenza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdVGNixov6Ul",
        "outputId": "c0c4cbc4-4beb-4895-ccb2-cce7bba8a21c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('several', 'States') : 36\n",
            "2. ('*', '*') : 26\n",
            "3. ('appropriate', 'legislation') : 9\n",
            "4. ('whole', 'number') : 7\n",
            "5. (']', 'Section') : 7\n",
            "6. ('following', 'States') : 5\n",
            "7. ('following', 'article') : 5\n",
            "8. ('supreme', 'Court') : 4\n",
            "9. ('public', 'Ministers') : 4\n",
            "10. ('other', 'States') : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_bigrams(10, pos_tags=('ADJ', 'NOUN'), order='cond_prob') # top 10 bigrammi (composti da aggettivo e sostantivo) per probabiità condizionata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0GvmBe_wSEW",
        "outputId": "cc4ac5b0-bb94-437d-a7d2-4d1d5da5e22f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('seventeenth', 'amendment') : 1.0\n",
            "2. ('previous', 'condition') : 1.0\n",
            "3. ('legislative', 'Powers') : 1.0\n",
            "4. ('sixteenth', 'amendment') : 1.0\n",
            "5. ('fourteenth', 'amendment') : 1.0\n",
            "6. ('appropriate', 'legislation') : 1.0\n",
            "7. ('commercial', 'regulations') : 1.0\n",
            "8. ('foregoing', 'Powers') : 1.0\n",
            "9. ('perfect', 'Union') : 1.0\n",
            "10. ('superior', 'figures') : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_bigrams(10, pos_tags=('ADJ', 'NOUN'), order='joint_prob') # top 10 bigrammi (composti da aggettivo e sostantivo) per probabiità congiunta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMCqoq7owiWK",
        "outputId": "c7188a13-c95d-41e2-837a-31cda34ab5ab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('several', 'States') : 0.0017748853719863926\n",
            "2. ('*', '*') : 0.001281861657545728\n",
            "3. ('appropriate', 'legislation') : 0.00044372134299659814\n",
            "4. (']', 'Section') : 0.0003451166001084653\n",
            "5. ('whole', 'number') : 0.0003451166001084652\n",
            "6. ('following', 'article') : 0.0002465118572203323\n",
            "7. ('following', 'States') : 0.0002465118572203323\n",
            "8. ('public', 'Ministers') : 0.00019720948577626584\n",
            "9. ('supreme', 'Court') : 0.00019720948577626584\n",
            "10. ('same', 'State') : 0.00014790711433219938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_bigrams(10, pos_tags=('ADJ', 'NOUN'), order='mi') # top 10 bigrammi (composti da aggettivo e sostantivo) per mutua informazione"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqAx_-OlwmYJ",
        "outputId": "a76d92de-eba6-426a-b937-3002479af67e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('previous', 'condition') : 19.76109848180614\n",
            "2. ('commercial', 'regulations') : 19.76109848180614\n",
            "3. ('superior', 'figures') : 19.76109848180614\n",
            "4. ('unusual', 'punishments') : 19.76109848180614\n",
            "5. ('net', 'Produce') : 19.76109848180614\n",
            "6. ('forty-eight', 'hours') : 19.76109848180614\n",
            "7. ('suppress', 'Insurrections') : 19.76109848180614\n",
            "8. ('unanimous', 'conviction') : 19.76109848180614\n",
            "9. ('good', 'Behaviour') : 19.76109848180614\n",
            "10. ('technical', 'correction') : 19.76109848180614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.top_k_bigrams(10, pos_tags=('ADJ', 'NOUN'), order='lmi') # top 10 bigrammi (composti da aggettivo e sostantivo) per mutua informazione locale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs4NV_x1wwJj",
        "outputId": "0927f566-e6cc-43bc-9fa2-bad507498f40"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ('several', 'States') : 423.0289260388173\n",
            "2. ('*', '*') : 270.8368669211484\n",
            "3. ('appropriate', 'legislation') : 147.95253348226902\n",
            "4. ('whole', 'number') : 105.53620215273848\n",
            "5. (']', 'Section') : 84.29367915309024\n",
            "6. ('following', 'article') : 65.20683712528144\n",
            "7. ('public', 'Ministers') : 64.70454392433993\n",
            "8. ('supreme', 'Court') : 63.81497423899415\n",
            "9. ('three-', 'fourths') : 51.52840794325495\n",
            "10. ('following', 'States') : 51.03830588025865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.common_mi_lmi(10, ('ADJ', 'NOUN'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6w5oZF-wy_N",
        "outputId": "71f35f86-f300-4fd0-9ba5-1b786e6d8838"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non ci sono bigrammi in comune nei primi 10 bigrammi con pos ('ADJ', 'NOUN') ordinati per mutual information e local mutual information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.analyze_sentences()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U24rKASuw9CI",
        "outputId": "3bfb444e-f34f-44cd-e772-492dabdbc1bc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase con la media della distribuzione di frequenza dei token più alta (495.05): \"The Congress shall have the power to enforce, by \n",
            "appropriate legislation, the provisions of this article.\".\n",
            "Frase con la media della distribuzione di frequenza dei token più bassa (120.94): \"The dates \n",
            "set out in this document are based upon the best information \n",
            "available.]\".\n",
            "Frase con la probabilità più alta secondo un modello di Markov di ordine 2 (2.2559210720711727e-28): \"Congress shall have power to enforce this \n",
            "article by appropriate legislation.\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze.ner(15) # top 15 NE più frequenti per ciascuna classe di NE con relativa frequenza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_Ox6t6PxAWn",
        "outputId": "d71030ad-a6aa-42d1-a673-97ab262d03dd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORGANIZATION\n",
            "1. Congress : 107\n",
            "2. House : 47\n",
            "3. Senate : 41\n",
            "4. Representatives : 33\n",
            "5. States : 16\n",
            "6. General Services : 10\n",
            "7. Legislature : 9\n",
            "8. Constitution : 8\n",
            "9. Consent : 7\n",
            "10. District : 7\n",
            "11. THE : 6\n",
            "12. Senators : 6\n",
            "13. Citizens : 6\n",
            "14. Validity Publication : 6\n",
            "15. Houses : 5\n",
            "\n",
            "GPE\n",
            "1. United States : 119\n",
            "2. New York : 23\n",
            "3. Virginia : 22\n",
            "4. Massachusetts : 21\n",
            "5. New Hampshire : 21\n",
            "6. Pennsylvania : 20\n",
            "7. Maryland : 20\n",
            "8. New Jersey : 18\n",
            "9. Vermont : 18\n",
            "10. South Carolina : 16\n",
            "11. Ohio : 16\n",
            "12. Georgia : 15\n",
            "13. North Carolina : 15\n",
            "14. Delaware : 15\n",
            "15. Missouri : 15\n",
            "\n",
            "PERSON\n",
            "1. Article : 25\n",
            "2. Rhode Island : 20\n",
            "3. Michigan : 15\n",
            "4. Minnesota : 15\n",
            "5. Louisiana : 12\n",
            "6. Montana : 12\n",
            "7. Law : 11\n",
            "8. Person : 8\n",
            "9. Power : 6\n",
            "10. Treason : 5\n",
            "11. Bill : 5\n",
            "12. Money : 4\n",
            "13. Illinois : 4\n",
            "14. Delaware : 3\n",
            "15. States : 3\n",
            "\n",
            "GSP\n",
            "1. Connecticut : 21\n",
            "2. Consequence : 3\n",
            "3. Philadelphia : 1\n",
            "\n",
            "LOCATION\n",
            "1. West Virginia : 14\n",
            "2. North Carolina : 1\n",
            "\n"
          ]
        }
      ]
    }
  ]
}